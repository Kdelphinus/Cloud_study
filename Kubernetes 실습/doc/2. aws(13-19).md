# Run Kubernetes in AWS Cloud (Section 13 ~ 19)

## Section 13: Moving to AWS

- EC2 인스턴스, 로드 밸런스, 오토 스케일 그룹을 쿠버네티스가 자동으로 생성하기에 따로 만들지 않을 것이다.
- 우리가 해야할 과제는 프리티어를 넘어서기에 실습이 끝나면 바로바로 리소스를 삭제해줘야 한다.
- 강사가 이틀을 돌려서 13,000원 정도 나왔으니 이것이 최대치일 것

### 사전 지식

- 클라우드 환경에서 제작을 할 때, 노드 몇 개를 만들어야 한다.
  - 노드는 우리 시스템에 있는 물리적 서버이다.
  - AWS에 있는 노드는 EC2 인스턴스이다.
  - 우리는 이를 클라우드에 있는 서버라고 생각하면 된다.
  - 노드 하나에 모든 아키텍처가 들어가지는 않으며 하나의 노드에 시스템의 일부만 배포할 것이다.
- 우리는 3개의 노드를 사용하여 배할 것이다.
  - API Gateway 가 속한 노드
  - Position Tracker, fleetman-mongodb 가 속한 노드
  - Position Simulator, Active Mq 가 속한 노드
- 쿠버네티스를 썼을 때 장점
  - 쿠버네티스는 **마스터 노드** 라는 상위 노드가 있어서 포드들을 스케줄링한다.
  - 적은 작업량으로 적게 생각하고도 시스템이 실패에 대응하도록 할 수 있다.
    - 예를 들어 API Gateway가 속한 노드가 크래시했다면 API Gateway 서비스가 사라진다.
    - 이때 마스터 노드는 이를 알아차리고 다른 노드에 API Gateway를 스케줄링한다.

- 적절한 쿠버네티스 클러스터는 이와 같이 노드들의 집합으로 이루어져 있다.
- 그리고 마스터 노드를 제외한 노드를 워커노드라고 부르기도 한다.
- 마스터 노드는 여러 개일 수 있지만 우리는 하나만 사용해도 충분하다.
- 클러스터 관리를 위해 대체로 [EKS](https://aws.amazon.com/ko/eks/)나 [kops](https://github.com/kubernetes/kops) 중 하나를 사용한다. 둘의 차이는 거의 없지만 그 중 가장 큰 차이는 마스터 노드와 관련된 것이다.

|        | kops                                                                                    | EKS                                                                                                                                                     |
|:-------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------|
| 제공     | kubernetes                                                                              | AWS                                                                                                                                                     |
| 마스터 노드 | - 직접 관리<br/>- 마스터 노드 통제 가능<br/>- 클러스터를 운영한다면 따라야 할 의무와 책임이 있음                           | - 백그라운드에서 돌아가며 아마존에서 관리<br/>- 볼 수도 없고 종료할 수도 없음<br/>- 즉, 아마존이 다 알아서 관리                                                                                  |
| 장점     | - 많이 쓰였다.<br/>- 사용하기 쉽다.                                                                | - 마스터 노드를 관리해준다.<br/>- 사용자가 계속해서 증가하고 있다.<br/>- eskctl 이라는 사용하기 쉬운 도구를 제공한다.<br/>- 서비스 관리와 인스턴스 없이도 클러스터를 생성할 수 있다(Fargate 기능이 통합됨)<br/>- 멀티 마스터를 제공한다. |
| 단점     | - 마스터 노드를 직접 관리해야 한다. 즉 마스터 노드에 대한 책임을 져야 한다.<br/>- 마스터 노드가 크래시나면 일시적으로 클러스터를 관리할 수 없다. | - EKS를 사용하려면 서드 파티 도구가 필요하다.<br/>- GUI가 좋지 않다.(2020년 기준)                                                                                                |

- 하지만 둘의 차이가 거의 없다. 위에 장단점은 강사가 말했듯이 굳이굳이 비교할 부분을 찾은 것일 뿐이다.

#### EKS와 kops의 비용 차이

- 워커노드는 어떤 것을 사용하든 차이가 없다.
- 마스터 노드, 로드 밸런서(마스터 노드에 접근하기 위해 필요) 관리 방법이 차이가 있다. 이를 **컨트롤 플레인**이라고 한다.
  - kops: 마스터 노드가 생성되면 콥스가 마스터 노드 옆에 로드 밸런서를 만든다.
  - EKS: 마스터 노드와 마찬가지로 로드 밸런서 역시 아마존에서 알아서 관리한다.

- 비용 비교(워커 노드를 뺀 비용)
  - kops
    - 프로젝트에 맞춰서 노드 타입을 선택해야 한다. 여기선 중간 단계의 노드로 택했다.
    - 로드 밸런서 역시 선택해야 한다.
    - 싱글 마스터일 때 2020년 기준 대략 1년에 99만원 정도이다.
    - 멀티 마스터가 된다면 가격이 적어도 3배 정도 뛸 것이다.
  - EKS
    - 전체 컨트롤 플레인에 set 비용을 지불한다.
    - 2020년 기준 대략 1년에 108만원 정도이다.
    - 하지만 EKS는 멀티 마스터를 제공한다.

## Section 14: KOPS - AWS 클라우드에서 쿠버네티스 실행

1. ec2 인스턴스 생성하기
- 이때 key-pair를 사용할 때 권한을 제거해줘야 한다.
```shell
> chmod 400 {key-pair file}
```
- 그리고 aws 대시보드에서 public ip를 참고하여 ssh로 접속한다.
```Shell
> ssh -i {key-pair file} ec2-user@{public ip}
```

2. kops 및 kubectl 설치

- [kops 설치 문서](https://kops.sigs.k8s.io/getting_started/install/)를 보고 설치
- [kubectl 설치 문서](https://kops.sigs.k8s.io/install/)를 보고 kubectl 설치

3. kops 사용자 생성 및 권한 부여

- kops 관리자를 aws IAM 서비스에서 만든다.
- [kops on AWS 공식문서](https://kops.sigs.k8s.io/getting_started/aws/)에서 필요한 권한을 부여한 그룹을 만들고 그 그룹에 사용자를 할당한다.
- `aws configure` 명령어로 유저를 추가한다.
  - access key와 secret key는 유저의 액세스, 시크릿 키를 사용한다.
  - 리전은 사용하고 있는 리전의 키를 입력한다. 예를 들어 서울은 `ap-northeast-2` 이다.
  - 다음 설정은 엔터로 넘어간다.
- `aws iam list-users` 명령어를 입력하면 json 형식의 유저 리스트를 확인할 수 있다.

4. 클러스터 생성

- [kops on AWS 공식문서](https://kops.sigs.k8s.io/getting_started/aws/)에서 Creating your first cluster 항목으로 간다.
- 현재 프로젝트에서는 gossip-based cluster를 생성할 예정이다. 그렇기에 이름이 `k8s.local` 로 끝나야 한다.
- 아래처럼 환경변수를 설정한다.

```Shell
> export NAME=fleetman.k8s.local
> export KOPS_STATE_STORE=s3://{자신이 가진 S3 서비스의 이름}
```

- 만약 노드마다 다른 가용성 영역에 있다면 대응성이 더 올라갈 것이다.
- 이렇게 가용성 영역을 설정해야 한다.

```Shell
# region에서 사용 가능한 가용성 영역
aws ec2 describe-availability-zones --region {region code}
```

- 이제 클러스터에 환경설정 파일을 만든다.
- 클러스터 이름은 아까 저장한 환경변수를 활용한다.

```Shell
# Create cluster configuraton(여러 개 가능, ','로 구분)
kops create cluster --name ${NAME} --zones {zonename},{zonename}
```

> 만약 ssh 에러가 발생하면 ssh 키 만들기

- cluster의 설정을 바꿀 수 있지만 default값을 그대로 사용해도 무관하다.

```Shell
# Customize Cluster Configuration
kops edit cluster --name ${NAME}
```

5. 노드 개수 관리

- kops는 이러한 인스턴스들을 관리하기 위해 인스턴스 그룹을 제공한다.

```Shell
# 인스턴스 그룹 목록
> kops get ig --name ${NAME}
NAME				ROLE		MACHINETYPE	MIN	MAX	ZONES
control-plane-ap-northeast-2a	ControlPlane	t3.medium	1	1	ap-northeast-2a
nodes-ap-northeast-2a		Node		t3.medium	1	1	ap-northeast-2a
nodes-ap-northeast-2b		Node		t3.medium	1	1	ap-northeast-2b
```

- 위처럼 콥스는 기본적으로 자동 설정해준다.
- `control-plane-ap-northeast-2a`
  - 이 인스턴스 그룹은 마스터 노드(나같은 경우는 컨트롤 플레인)에 관한 설명이다.
  - 최소와 최대는 실행될 수 있는 인스턴스의 범위로 보면 된다.
- `nodes-ap-northeast-2a` , `nodes-ap-northeast-2b`
  - 콥스가 디폴트로 워커 노드를 설정한 것
  - 콥스는 각 가용성 영역에서 분리된 인스턴스 그룹을 만든다.

- 만약 인스턴스의 개수를 조정하고 싶다면 인스턴스 그룹을 수정하면 된다.

```Shell
# 인스턴스 그룹 수정
kops edit ig {ig name} --name ${NAME}
```

- 위 명령어를 치면 에디터로 환경파일을 수정할 수 있다.
- 강의는 3개의 노드가 필요하기에 서울의 경우 한 영역에는 1개, 다른 영역에는 2개를 구성해야 한다.

6. 클러스터 실행

```Shell
# Build the Cluster
> kops update cluster --name ${NAME} --yes --admin

# 관리자가 접근할 수 있는 시간 설정(2021년 기준 기본값은 18시간, 87600h = 대략 10년)
> kops update cluster --name ${NAME} --yes --admin=87600h
```

- 실행되는데 시간이 꽤나 걸린다.
- 만약 시간을 변경했다면 kubctl 환경설정 역시 바꿔줘야 한다.

```Shell
# 만약 관리자가 접근할 수 있는 시간을 바꿨다면
> kops export kubecfg --admin={설정시간}
```

- 클러스터가 실행되는지 확인한다.

```Shell
> kops validate cluster
```

- 에러가 뜨면 아직 시작 중이라는 것을 의미한다.
- `validation Failed` 역시 마찬가지다.
- 여러 정보들이 뜨고 마지막에 `Your cluster fleetman.k8s.local is ready` 가 뜨면 준비가 완료된 것이다.
- ec2에 노드가 더 생기고 로드 밸런서 역시 생성된 것을 확인할 수 있다.
- 로드 밸런서
  - 로드 밸런서는 트래픽을 인스턴스에 지시하는 방법이다.
  - 로드 밸런서에는 안정적인 DNS 이름이 있다.
  - kubectl 같은 명령어를 입력할 때, 로드 밸런서를 호출하고 로드 밸런서는 마스터 인스턴스를 지시한다.
- 아마존에서 생성된 인스턴스는 오토 스케일링 그룹에 의해 보호된다. 즉 충돌이 나도 오토 스케일링 그룹의 설정에 따라 자동으로 재시작된다.
- 

## Section 15: EKS - AWS 클라우드에서 쿠버네티스 실행

1. ec2 인스턴스 생성하기
- 이때 key-pair를 사용할 때 권한을 제거해줘야 한다.
```shell
> chmod 400 {key-pair file}
```
- 그리고 aws 대시보드에서 public ip를 참고하여 ssh로 접속한다.
```Shell
> ssh -i {key-pair file} ec2-user@{public ip}
```

2. eksctl 설치

- [AWS eksctl 설치 문서](https://docs.aws.amazon.com/ko_kr/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-eksctl.html)에서 linux 버전 참고하기
- 2024.05.31 버전 명령어

```Shell
# 다음 명령으로 eskctl 최신 릴리스 다운 및 압축 해제
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
```

```Shell
# 추출된 바이너리를 /usr/loacl/bin 으로 이동
sudo mv /tmp/eksctl /usr/local/bin
```

```Shell
# 설치가 되었는지 확인. eksctl 0.34.0 이상의 버전이 있어야 함
eksctl version
```

> `aws` 명령어로 aws 서비스를 접근 및 실행할 수 있다.

3. eks 유저 그룹 및 유저 생성하기

- 그룹은 아래와 같은 권한이 필요하다(2024년 5월 기준)
  - AmazonEC2FullAccess
  - IAMFullAccess(필수는 아니지만 강의의 편의를 위해, 실제 환경에선 너무 광범위한 범위이기에 생각을 잘 해야 한다.)
  - AWSCloudFormationFullAccess
- 그 후, 그룹 권한에서 인라인 정책을 아래의 json으로 생성한다.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "eks:*",
      "Resource": "*"
    },
    {
      "Action": [
        "ssm:GetParameter",
        "ssm:GetParameters"
      ],
      "Resource": "*",
      "Effect": "Allow"
    }
  ]
}
```

- 위 정책은 과제 연습을 위한 간략화된 정책으로 실제 권한과 정책을 설정할 때는 [eksctl 공식 문서의 IAM 최소 권한 정책](https://eksctl.io/usage/minimum-iam-policies/)을 참고해서 권한을 부여하면 된다.
  - 크게 IAM 권한과 위에 있는 EKS 관련 권한이 공식 문서에서 권장하는 권한과 다르다.
- `aws configure` 명령어로 유저를 추가한다.
  - access key와 secret key는 유저의 액세스, 시크릿 키를 사용한다.
  - 리전은 사용하고 있는 리전의 키를 입력한다. 예를 들어 서울은 `ap-northeast-2` 이다.
  - 다음 설정은 엔터로 넘어간다.
- `aws iam list-users` 명령어를 입력하면 json 형식의 유저 리스트를 확인할 수 있다.
- `aws esk list-clustres` 명령어도 더 이상 에러가 발생하지 않는다. 물론 클러스터를 생성하진 않았기에 비어있는 것이 맞다.

> `aws eks` 명령어를 사용할 수 있지만 낮은 수준의 명령어가 많다. 그렇기에 이를 대신해서 `eksctl` 명령어를 사용한다.
> 물론 `eksctl` 명령어는 모두 `aws eks` 를 아주 많이 호출한다. 

4. kubectl 설치

- 클러스터를 생성하면 이전에 배웠던 것과 마찬가지로 `kubectl` 명령어로 관리할 것이다. 그렇기에 `kubectl` 설치해야 한다.
- [kubernetes 공식 문서](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)와 [kops 공식 문서](https://kops.sigs.k8s.io/install/)를 참고해서 설치하면 되는데 이때 eks에 있는 쿠버네티스 기본 버전을 고려해서 설치해야 한다.
  - eks 클러스터를 생성하진 않을거지만 생성 버튼을 누르면 쿠버네티스 버전이 보인다. 이를 통해 확인하면 된다.
  - 2024.05 기준으로 기본 버전은 1.29.0이다.

```Shell
# 특정 버전 설치
curl -LO https://dl.k8s.io/release/{specific version}/bin/linux/amd64/kubectl

# 1.29.0 버전 설치
curl -LO https://dl.k8s.io/release/v1.29.0/bin/linux/amd64/kubectl
```

- 밑에 명령어는 쿠버네티스 공식문서에서 root 권한이 없을 때 활용하는 방법을 가져온 것이다.

```Shell
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl
```

```Shell
# 설치되었는지 확인
kubectl version --client
```

5. 클러스터 생성 및 시작

- [eksctl 문서](https://eksctl.io/usage/creating-and-managing-clusters/)와 강의 참고

```Shell
# cluster의 이름과 최소 워커 노드를 지정하는 옵션 추가
eksctl create cluster --name {cluster 이름} --nodes-min=3
```

- 조금 시간이 걸린다. 
- 끝나면 EKS 서비스에 클러스터가, EC2 서비스에 워커 노드들이 생성되어 있는 것을 확인할 수 있다.
- 이때, kops와 다르게 마스터 노드를 볼 수 없는 것을 확인할 수 있다.

## Section 16: Cluster operate

### 스토리지 클래스로 프로비저닝

- `kubectl get all` 명령어로 정상 동작 중인 클러스터를 확인할 수 있다.
- 우선 `storage.yaml` 파일에서 저장할 볼륨의 위치를 aws에 맞게 변경해야 한다.
- 이를 위해 [kubernetes의 storage classes](https://kubernetes.io/docs/concepts/storage/storage-classes/)를 사용한다.
  - [AWS EBS의 storage classes](https://kubernetes.io/docs/concepts/storage/storage-classes/#aws-ebs)를 확인하면 2024년 5월기준 아래와 같은 예시가 있다
  ```yaml
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: ebs-sc
  provisioner: ebs.csi.aws.com
  volumeBindingMode: WaitForFirstConsumer
  parameters:
    csi.storage.k8s.io/fstype: xfs
    type: io1
    iopsPerGB: "50"
    encrypted: "true"
  allowedTopologies:
  - matchLabelExpressions:
    - key: topology.ebs.csi.aws.com/zone
      values:
      - us-east-2c
  ```

- 우리는 위에 있는 모든 걸 사용하지 않고 아래와 같이 간략하게 사용했다.

```yaml
# What do you want? (무엇을 원하는가?)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongo-pvc
spec:
  storageClassName: cloud-ssd  # StorageClass 이름 변경에 따른 이름 변경
  accessModes:
    - ReadWriteOnce  # 한 번에 다중 노드에 의해 마운트될 수 있는 볼륨을 원하는지에 대한 옵션
  resources:
    requests:
      storage: 7Gi  # db가 필요한 용량(물리적 저장소에 요구할 용량)

---

# How do we want it implemented? (어떻게 실행할 것인가?)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: cloud-ssd  # 이름 변경
provisioner: ebs.csi.aws.com
parameters:
  type: gp2  # io1보다 저렴한 버전
```

- 2024년 5월 기준 EKS 버전은(kops 사용 시, 건너뛰기) 쿠버네티스 클러스터가 EBS에 액세스할 수 있도록 드라이버를 설치해야 한다. 이를 위해 아래의 명령어를 먼저 실행한다.

```Shell
eksctl utils associate-iam-oidc-provider --region={Region name} --cluster={Cluster name} --approve
```

```Shell
eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster {Cluster name} --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve  --role-only  --role-name AmazonEKS_EBS_CSI_DriverRole
```

```Shell
eksctl create addon --name aws-ebs-csi-driver --cluster {Cluster name} --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole --force
```

- 이후, 기존처럼 `kubectl apply` 명령어를 사용하면 pv와 pvc 모두 확인할 수 있다.

```Shell
> kubectl apply -f storage-aws.yaml
persistentvolumeclaim/mongo-pvc created
storageclass.storage.k8s.io/cloud-ssd created
> kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
mongo-pvc   Bound    pvc-203eca62-19c4-4f91-9d2f-551501784da6   7Gi        RWO            cloud-ssd      <unset>                 5m45s
> kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-203eca62-19c4-4f91-9d2f-551501784da6   7Gi        RWO            Delete           Bound    default/mongo-pvc   cloud-ssd      <unset>                          11s
```

- pv와 pvc 모두 상태는 **Bound** 여야 한다. 그리고 ec2 서비스 볼륨에서 **사용가능 상태**로 생성된 것을 확인할 수 있다.
- pv 값들 중 `RECLAIM POLICY` 는 pvc가 삭제될 때, pv에 대한 처리를 나타낸 옵션으로 기본값은 pv도 같이 삭제하는 `Delete` 이다.

### 워크로드 배포

- `mongo-stack.yaml` 파일을 배포하면 위에서 만든 볼륨이 **사용 가능** 에서 **사용 중** 으로 변한 것을 확인할 수 있다.
- `service.yaml` 과 `workloads.yaml` 파일을 각각 옮기고 `service.yaml` 파일을 약간 수정한다.
  - `webapp` 에서 `nodePort` 로 직접 접근가능하게 했던 것을 `LoadBalancer` 로 수정한다. 이를 통해 로드 밸런서로 접근하게 만든다. 이는 로드 밸런서가 있는 클러스터 환경에서만 가능하다.
  - 또한 디버깅을 위해 `nodePort` 로 설정한 서비스들은 `ClusterIP` 로 변경한다.
- 그 뒤, `service.yaml` 과 `workloads.yaml` 파일을 배포한다. 이때 `kubectl apply -f .` 명령어를 사용해도 새로 적용되는 두 파일만 배포될 것이기에 써도 무방하다.

- ec2 서비스의 로드 밸런서 탭에 들어가면 로드 밸런서가 적어도 하나 이상은 있을 것이다.
  - kops가 추가로 갖는 로드 밸런서
    - api로 시작되는 로드 밸런서
    - 이는 마스터 노드를 위한 것
    - kubectl 명령어가 곧바로 로드 밸런서에게 간다.
  - eks와 kops가 공통으로 갖는 로드 밸런서
    - 자동으로 생성된 이름을 가진 로드 밸런서
    - 로드 밸런서를 체크해서 '대상 인스턴스'를 확인하면 연결되어 있는 3개의 인스턴스를 확인할 수 있다.
    - 세부 정보에서 'DNS 이름'이 우리의 도메인 이름이다. 이를 접근하면 우리의 서비스로 접근할 수 있다.

### 도메인 이름 설정

- 도메인 이름은 자동 생성되어서 보기 좋지 않다. 그렇기에 실제 서비스에선 소유한 도메인 이름으로 변경해야 한다.
  - AWS 서비스 중 도메인 이름을 관리하는 Route 53 서비스에 들어간다.
  - 도메인 생성을 누르고 일정 가격을 지불하고 도메인을 생성할 수 있다.
  - 로드 밸런서를 선택할 수 있는데 이때 위에서 만들어진 eks와 kops가 공통으로 갖는 로드 밸런서를 선택하면 된다.

### 노드가 어떠한 이유에서라도 종료되었을 떄도 서비스는 지속되는가

- 다른 노드들이 실패해도 webapp은 정상 동작하는지 확인할 계획이다.

```Shell
# 어떤 파드가 실행되는지 확인
> kubectl get pods -o wide
NAME                                  READY   STATUS    RESTARTS   AGE   IP               NODE                                                NOMINATED NODE   READINESS GATES
api-gateway-56c46fbcdb-b92rp          1/1     Running   0          21m   192.168.64.13    ip-192-168-76-58.ap-northeast-2.compute.internal    <none>           <none>
mongodb-57bd58556b-rkzml              1/1     Running   0          27m   192.168.37.171   ip-192-168-41-2.ap-northeast-2.compute.internal     <none>           <none>
position-simulator-5fdb4ddbd5-8b5fj   1/1     Running   0          21m   192.168.83.107   ip-192-168-76-58.ap-northeast-2.compute.internal    <none>           <none>
position-tracker-59fdfd8cf4-m9spw     1/1     Running   0          21m   192.168.29.131   ip-192-168-21-210.ap-northeast-2.compute.internal   <none>           <none>
queue-7f49849f96-sxnf6                1/1     Running   0          21m   192.168.31.168   ip-192-168-21-210.ap-northeast-2.compute.internal   <none>           <none>
webapp-66765b68df-b7bkl               1/1     Running   0          21m   192.168.28.227   ip-192-168-21-210.ap-northeast-2.compute.internal   <none>           <none>
```

- webapp이 속한 노드를 종료시키면 서비스에 접속할 수 없다.
- 하지만 쿠버네티스가 정상 동작 중인 노드에 포드를 리스케줄하기에 서비스 중단의 기간이 길지는 않다.
- 이는 정확히는 오토 스케일링 그룹이 동작한 것으로 봐야 한다.
- 오토 스케일링 그룹의 활동 부분을 보면 무슨 일이 일어났는지 자세히 확인할 수 있다.
- 하지만 사용자가 불편을 느끼기에 충분히 긴 시간이었다.

### 포드 복제

- 우리는 webapp 포드 하나가 종료되어도 서비스의 문제가 없기를 원한다. 그렇기에 아래와 같이 `workloads.yaml` 파일을 수정하여 포드를 복제한다.

```yaml
# workloads.yaml
...
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  selector:
    matchLabels:
      app: webapp
  replicas: 2  # 포드를 2개 생성
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
        - name: webapp
          image: richardchesterwood/k8s-fleetman-webapp-angular:release2
          env:
            - name: SPRING_PROFILES_ACTIVE
              value: production-microservice
```

- 이를 배포하고 pod를 확인하면 다음과 같이 다른 가용 영역에서 배포된 webapp 포드들을 확인할 수 있다.

```Shell
> kubectl get pods -o wide
NAME                                  READY   STATUS    RESTARTS   AGE    IP               NODE                                               NOMINATED NODE   READINESS GATES
api-gateway-56c46fbcdb-b92rp          1/1     Running   0          36m    192.168.64.13    ip-192-168-76-58.ap-northeast-2.compute.internal   <none>           <none>
mongodb-57bd58556b-rkzml              1/1     Running   0          43m    192.168.37.171   ip-192-168-41-2.ap-northeast-2.compute.internal    <none>           <none>
position-simulator-5fdb4ddbd5-8b5fj   1/1     Running   0          36m    192.168.83.107   ip-192-168-76-58.ap-northeast-2.compute.internal   <none>           <none>
position-tracker-59fdfd8cf4-knsj7     1/1     Running   0          10m    192.168.72.72    ip-192-168-76-58.ap-northeast-2.compute.internal   <none>           <none>
queue-7f49849f96-fwkfw                1/1     Running   0          10m    192.168.84.64    ip-192-168-76-58.ap-northeast-2.compute.internal   <none>           <none>
webapp-66765b68df-ngsmn               1/1     Running   0          10m    192.168.53.30    ip-192-168-41-2.ap-northeast-2.compute.internal    <none>           <none>
webapp-66765b68df-rztdm               1/1     Running   0          106s   192.168.24.168   ip-192-168-9-21.ap-northeast-2.compute.internal    <none>           <none>
```

- 여기서 하나의 webapp 포드가 죽어도 서비스는 거의 지장이 없다.

- 하지만 큐나 시뮬레이터, 몽고같은 서비스는 포드가 죽으면 데이터를 바로 얻을 수 없다.
- 그렇다고 포드를 복제해서 만들면 데이터가 복제되는 현상이 발생한다.
- 즉, Stateful한 포드들은 복제할 수 없다. 오직 Stateless한 포드들만 복제할 수 있다.
- 이에 대한 해결방안으로 aws 등에서 제공하는 서비스를 이용하여 포드 자체를 제거하거나 하는 방법이 있다. 이에 대해선 강의에서 자세히 다루지는 않는다.

> Stateful(상태유지): 서버가 클라이언트의 상태를 보존
> Stateless(무상태): 서버가 클라이언트의 상태를 보존하지 않음
> - [참고자료](https://velog.io/@adc0612/stateful%EA%B3%BC-stateless-%EC%B0%A8%EC%9D%B4-7tfkp7a4)


## Section 17: Delete cluster in Kops

### 클러스터 삭제

- 클러스터 삭제는 아래의 명령어로 가능하다.
- 이 명령어는 `${NAME}` , `${KOPS_STATE_STORE}` 으로 설정한 2개의 환경 변수 세트에 의존한다.
  - 만약 서버를 나갔다가 돌아온다면 환경변수는 지워져있다.
- 그렇기에 삭제 하기전에 꼭 환경변수가 있는지 확인부터 해야 한다.
- 환경변수 복구 방법
  - `history | grep export` 명령어 사용 -> `!{명령어 번호}` 를 사용하면 된다.
  - 만약 쉘 밖에서 인스턴트에 크래시가 발생한다면 history가 없다. 이때는 다시 직접 입력해야 한다.

- 위 모든게 확인되었다면 명령어를 실행한다.

```Shell
> kops delete cluster --name ${NAME} --yes
```

- 그리고 스크립트가 잘 끝났는지 확인하고 aws에 가서 확인한다.
  - 인스턴스가 잘 끝났는지(인스턴스는 종료되도 사라지는데 시간이 걸린다)
  - 볼륨이 하나만 남았는지
  - 로드 밸런서가 없는지
  - 오토 스케일링 그룹이 없는지


- 만약 인스턴스를 나중에 다시 사용할거라면(예를 들어 아직 강의를 덜 들었다면) 인스턴스 상태를 `STOP(중지)` 상태로 바꾼다.
  - 이때, 볼륨은 멈추지 않기 때문에 멈춰도 한 달에 몇 백원 정도 비용은 발생한다.
- 다시 사용하지 않을거고 볼륨까지 종료하고 싶다면 인스턴스를 `Terminate(종료)` 상태로 바꾸면 된다.

### 클러스터 재시작

- 먼저 인스턴스를 `Start(시작)` 상태로 변경한다.
- 인스턴스의 보안탭에서 인바운드를 살펴보고 만약 동적 ip 주소를 가지고 있다면 편집으로 내 ip로 다시 변경해줘야 한다. 
- 인스턴스에 접속한 후, 먼저 환경변수를 재설정한다.
- 클러스터 생성 과정부터 yaml 파일을 적용하는 것까지 쭉 다시 진행하면 된다.

## Section 18: Delete cluster in EKS

```Shell
eksctl delete cluster {Cluster name}
```

- 명령어는 몇 분 정도 걸리며 명령어가 끝나고 aws에서 제대로 삭제되었는지 확인해봐야 한다.
  - 로드 밸런서가 없는지
  - 인스턴스에 우리가 만든 인스턴스만 남았는지
    - 종료는 삭제된 것과 같다.
    - 종료 후 목록에서 사라지는데 좀 걸린다.
  - EKS 서비스 목록에서 클러스터가 없는지
  - 볼륨은 직접 삭제해야 한다.
    - EKS가 자동으로 만든 EBS 볼륨을 삭제(사용 가능 상태인 볼륨이다.)
    - 우리가 생성한 인스턴스의 볼륨은 남겨둔다.
- 이후, 인스턴스를 다시 사용할거면 **중지**, 사용하지 않을거면 **종료**한다.
  - 이때 중단하면 볼륨은 남아있고 월 몇 천원 정도의 비용을 발생시킨다.
  - 종료하면 볼륨도 같이 삭제되니 이마저도 싫으면 인스턴스를 삭제하면 된다.
- 재시작 시, eksctl을 실행하고 클러스터를 다시 만들기만 하면 된다.

> [Google Cloud Platform에 배포하기](https://www.youtube.com/watch?v=w9vy3wHGKeo)에 대한 강의