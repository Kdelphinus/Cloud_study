# Kubernetes and Microservices (~ Section 12)

## Section 3: Kubernetes

### Orchestration

- 컴퓨터 시스템, 미들웨어, 서비스 등을 자동화하고 조정하는 컴퓨터 소프트
- 컨테이너 오케스트레이션은 컨테이너화된 애플리케이션을 배포, 관리, 확장하는 일련의 프로세스를 자동화하는 것을 의미
- 특정 컨테이너가 항상 실행 중이도록 만들 수 있다.
- 또한 어떠한 에러가 발생하더라도 컨테이너가 다시 시작되도록 할 수 있다.


### Kubernetes(K8S)

- 컨테이너 오케스트레이션 도구 중 하나
- Kubernetes를 위한 [다양한 도구들](https://kubernetes.io/docs/tasks/tools/)이 있다.
    - minikube: 로컬 환경에서 Kubernetes 클러스터를 구축할 수 있는 도구
    - kubectl :Kubernetes 클러스터와 상호작용하기 위한 커맨드 라인 도구

> ### 주의사항(docker가 이미 설치되어 있다면)
> - minikube 내부에 자체 docker 설치가 함께 제공되기에 docker를 설치할 필요가 없다.
> - 만약 이미 docker를 설치했다면 도커 드라이버를 사용하면 된다.
> - minikube 설치를 완료하고 `minikube start --memory 4096 --driver docker` 명령어로 minikube를 실행하면 된다.
> - 또한 docker desktop이 있다면 minukube 말고 docker desktop을 사용해서 진행할 수 있다.


### Kubernetes Cluster

- Kubernetes 클러스터는 애플리케이션 컨테이너를 실행하기 위한 일련의 노드 집합이다.
- Kubernetes를 실행 중이라면 클러스터를 실행하고 있는 것이다.
- 클러스터는 여러 노드로 구성되어 있으며, 각 노드는 Kubernetes의 일부이다.
- 클러스터는 물리 머신, 가상 머신, 온프레미스, 클라우드에 구애받지 않고 머신 그룹 전체에서 컨테이너를 예약하고 실행할 수 있다.
- 쿠버네티스 컨테이너는 개별 머신에 연결되지 않고 클러스터 전체에서 추상화된다.
- 관련 용어(강의를 통해 자세히 학습할 예정)
  - control plane: 쿠버네티스 노드를 제어하는 프로세스의 모음이다. 여기에서 모든 테스크 할당이 시작된다.
  - node: 컨트롤 플레인에서 할당된 요청 태스크를 수행하는 머신이다.
  - pod: 쿠버네티스에서 가장 작은 배포 단위이다. 단일 노드에 배포되는 하나 이상의 컨테이너 집합이다.
  - service: 일련의 포드에서 네트워크 서비스로 실행 중인 애플리케이션을 노출하는 방식이다. pod의 집합에 대한 네트워크 엔드포인트를 정의한다.
  - volume
    - 컨테이너에서 데이터를 저장하는 방법이다. 컨테이너가 종료되면 데이터가 사라지기 때문에 별도의 볼륨을 사용한다.
    - 쿠버네티스 볼륨은 이 볼륨을 묶는 포드의 수명과 동일하다.
    - 볼륨은 포드 내에서 실행되는 모든 컨테이너보다 오래 지속되며 컨테이너를 다시 시작해도 데이터는 보존된다.
  - namespace: 쿠버네티스 클러스터 내의 가상 클러스터이다. 이를 통해 하나의 물리 클러스터 내에 있는 여러 클러스터를 관리할 수 있다.
- [RedHat에서 제공하는 쿠버네티스 클러스터에 대한 간단한 설명](https://www.redhat.com/ko/topics/containers/what-is-a-kubernetes-cluster)


## Section 4: Kubernetes Pods

### Pods

- Kubernetes는 컨테이너의 시작과 종료를 총체적으로 관리한다고 할 수 있다.
- 이를 위해 Kubernetes는 아키텍쳐의 조직을 위해 여러 개념을 가지고 있는데 그 중 가장 기본 개념이 Pod이다.
- 각 컨테이너들에게 Pod를 만들어 일종의 컨테이너 포장지처럼 감싼다. 
- 그렇기에 pod와 컨테이너는 대부분 일대일 관계를 가진다.
    - 예외적으로 컨테이너의 동작을 보조하는 컨테이너를 같은 pod에 둘 수 있다.
    - 이를 **sidecar 컨테이너** 라고 한다.
    - 허나 하나의 pod가 두 개의 마이크로서비스를 갖는 것은 매우 드문 일이다.
- kubernetes는 이 pod를 관리하고, pod가 어떻게 동작하는지를 결정한다. 그렇기에 pod는 kubernetes의 기본으로 배포되는 유닛이다.
- pod는 쿠버네티스 클러스터 외부에서 확인될 수 없다. pod는 클러스터 내부에서만 확인할 수 있다.
- 관련 링크: [Pod 설명](https://kubernetes.io/docs/concepts/workloads/pods/), [Pod API](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/)

### Pod 구성 yaml 파일 예시

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod  # pod의 이름
spec:
containers:  # pod에 포함될 컨테이너 목록
  - name: ubuntu
    image: ubuntu:trusty
    command: ["echo"]
    args: ["Hello World!"]
```

### Pod 명령어

#### 현재 클러스터에 있는 모든 리소스 출력

```bash
# default 네임스페이스 안에 모든 리소스
kubectl get all

# 모든 네임스페이스 안에 모든 리소스
kubectl get all --all-namespaces
```

#### Pod 생성 및 변경 사항 적용

```bash
 # pod 생성
kubectl apply -f {pod-definition.yaml}
```

#### Pod 목록 출력
  
```bash
kubectl get pods

# 약어를 사용할 수도 있다.
kubectl get po

# label 같이 출력
kubectl get po --show-labels

# label 별로 출력
kubectl get po -l {label-name}
```

#### Pod 상세 정보 출력

```bash
kubectl describe pod {pod-name}
```

#### Pod 명령어 실행

```bash
kubectl exec {pod-name} {command}
```

#### Pod shell 접속

```bash
kubectl exec -it {pod-name} sh
```

#### Pod 삭제
```bash
 # pod 삭제
kubectl delete po {pod-name}

# 전체 삭제
kubectl delete po --all
```

> #### minikube ip 관련
> 
> - 도커 드라이버나 도커 데스크탑을 사용하면 `minikube ip` 명령어가 동작하지 않는다.
>   
> 
> ##### docker desktop 사용 시
> 
> - docker desktop을 사용하면 `localhost`로 접근할 수 있다.
> - 강의에서 주어진 port는 30080 포트이다. 따라서 `localhost:30080`으로 접근하면 된다.
> - 만약 동작하지 않는다면 `kubectl port-forward webapp 30080:80` 을 실행한 뒤, 접속하면 된다.
> - service를 사용하면 위 명령어를 사용하면 안 된다.
> - 정확히는 `kubectl port-forward {service name} {port}:{nodePort}` 이다.
> 
> 
> ##### docker driver 사용 시
> 
> - `minikube service fleetman-webapp` 명령어를 통해 접근할 수 있다.
> - 브라우저가 자동으로 열린다.
> - 만약 열리지 않는다면 출력 결과에 있는 url을 보고 접속해야 한다.

## Section 5: Kubernetes Services

### Service

#### Pod의 문제점
- pod는 짧은 수명을 가지기 때문에 pod가 종료되면 새로운 pod가 생성된다.
- 이 때 pod의 IP 주소가 변경되기 때문에 pod를 외부에서 접근할 수 없다.

#### Service
- 이를 해결하기 위해 Service를 사용한다.
- Service는 긴 수명과 IP 주소, 안정되고 고정적인 port를 갖는다.
- Service는 pod의 집합을 대표하며, pod의 집합을 논리적인 그룹으로 묶어 외부와 통신할 수 있게 한다.
- [Service API](https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/)

### Service와 Pod의 관계
- Pod는 Labels(임의의 key:value 값)를 가지고 있다.
- Service는 Selector(임의의 key:value 값)를 가지고 있다.
- Service는 Selector와 일치하는 Pod를 찾아 트래픽을 전달한다.

### Service 구성 yaml 파일 예시

```yaml
kind: Service
apiVersion: v1
metadata:
  # Unique key of the Service instance
  name: service-example  # 서비스 이름
spec:
  ports:
    # Accept traffic sent to port 80
    - name: http
      port: 80
      targetPort: 80
  selector:
    # Loadbalance traffic across Pods matching
    # this label selector.
    app: nginx # pod의 label
  # Create an HA proxy in the cloud provider
  # with an External IP address - "Only supported
  # by some cloud providers"
  type: LoadBalancer
```

- `type: LoadBalancer`는 클라우드 제공자에서만 지원한다.
- 로컬에서 사용하려면 아래 두 개중 하나를 사용하면 된다.
  - `type: ClusterIP`
    - 클러스터 내부에서만 접근 가능하다.
    - 주로 클러스터 내부에서 통신하고 외부 트래픽에 노출시키지 않을 때 사용한다.
  - `type: NodePort`
    - 클러스터 외부에서 접근 가능하다.
    - 서비스와 연결된 포드로 접근할 수 있도록 만든다.
    - 이때는 `ports` 블록 내부에 `nodePort`를 추가해야 한다.
    - `nodePort`는 외부에서 접근할 수 있는 포트를 설정하는 것이며 30000-32767 사이의 포트를 사용한다.

- 또한 service와 pod를 연결하기 위해 pod의 label을 설정해야 한다.

```yaml
...
metadata:
  name: webapp
  labels:
    app: webapp
...
```

- labels에 설정한 `app: webapp`은 service의 selector에 설정한 `app: webapp`과 일치해야 한다.
- labels의 키와 값은 자유롭게 설정할 수 있다.

### Service 명령어

#### Service 생성 및 변경사항 적용

```bash
kubectl apply -f {service-definition.yaml}
```

#### Service 목록 출력

```bash
kubectl get services

# 약어를 사용할 수도 있다.
kubectl get svc

# label 같이 출력
kubectl get svc --show-labels
```

#### Service 상세 정보

```bash
kubectl describe service {service-name}

# 약어를 사용할 수도 있다.
kubectl describe svc {service-name}
```

#### 현재 경로에 있는ㄴ yaml 파일 적용

```bash
kubectl apply -f .
```

### 새로운 버전의 Pod 배포

- 새로운 버전의 pod를 배포하기 위해 기존 pod를 지우고 새로운 pod를 생성하면 생성되는 기간동안 서비스 사용이 불가하다.
- 그렇기 때문에 기존 pod를 열어두고 새로운 pod를 생성한 뒤 서비스를 전환하는 방법을 사용한다.
- 이때 labels를 통해 pod를 전환한다.

```yaml
# first-pod.yaml
...
metadata:
  name: webapp
  labels:
    app: webapp
    release: "0"
...

---

...
metadata:
  name: webapp-release-0-5
  labels:
    app: webapp
    release: 0-5
...
```

기존 포드와 새로운 포드를 위와 같이 파일에 정의한 뒤, labels 값으로 버전을 하나 추가한다.

```yaml
# webapp-service.yaml
...
spec:
  selector:
    app: webapp
    release: "0" # -> "0-5"
...
```

서비스는 기존 포드를 바라보다가 새로운 포드가 배포되면 새로운 포드를 바라보도록 변경하면 된다.
`describe` 명령어를 통해 서비스의 labels를 확인할 수 있다.


## Section 8: Kubernetes ReplicaSets

### Pod의 한계

- 결론부터 말하면 실제 서비스에서는 Pod를 직접 사용하지 않는다.
- 왜냐하면 pod는 다양한 이유로 종료될 수 있기 때문이다.
- 그리고 직접 배포한 pod가 종료된다면 그 pod는 자동적으로 다시 재시작되지 않는다.

### ReplicaSets

- ReplicaSets는 Pod를 관리하는 객체이다.
- ReplicaSets는 Pod의 수를 관리하고, Pod가 종료되면 새로운 Pod를 생성한다.
- [ReplicaSet API](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/replica-set-v1/)
- ReplicaSets의 yaml 파일 내에는 Pod 관련 내용도 포함된다.

### ReplicaSet 구성 yaml 파일 예시

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  # Unique key of the ReplicaSet instance
  name: replicaset-example

spec:
  selector:
    matchLabels:
      app: nginx
  # 3 Pods should exist at all times.
  replicas: 3
  template:  # template for the pods
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.14.2
```

- template 내부에 pod의 내용을 작성한다.
- replica의 이름은 pod의 이름과 동일한 것을 사용하는 것이 좋다.
- 또한 pod는 replica의 관리를 받고 있기에 따로 이름을 가질 필요가 없다.
- replica는 동일한 label을 가진 포드를 관리하고 포드가 어떤 이유간에 종료되면 새로운 포드를 실행시킨다.

### ReplicaSet 명령어

- 기존 명령어와 동일하고 `replicaset` 혹은 `rs` 를 사용하면 된다.

### ReplicaSet 내부 pod가 종료될 때

- ReplicaSet는 Pod를 관리하기 때문에 Pod가 종료되면 새로운 Pod를 생성한다.
- 만약 `replicas` 필드의 값이 2라면 같은 pod가 2개 생성된다.
- 그리고 서비스는 네트워크의 엔드포인트, 즉 이 경우엔 방문할 브라우저를 나타내는데 엔드포인트가 두 개 이상의 포드에 서비스되고 요청에 응답하는 포드가 지속적으로 변경된다는 게 쿠버네티스의 동작이다.

### 공식문서의 권장사항

- 공식문서를 보면 대부분의 경우 ReplicaSet 대신 배포, 생성을 권장한다는 경고 문구가 있다.


## Section 9: Kubernetes Deployments

### Deployments(배포)

- 배포는 레플리카 세트를 관리하는 객체이다.
- 쿠버네티스에서 배포는 기본적으로 보다 정교한 형태의 레플리카 세트라고 할 수 있다.
- 레플리카 세트에서 중단 없이 자동 업데이트(롤링 업데이트)를 할 수 있는 기능을 추가한 것과 비슷하다.
- [Deployment API](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/deployment-v1/)

### 배포 중 문제가 생겼을 때

- 이미지 이름이 없거나 네트워크 문제가 생겨서 이미지를 가져오지 못할 때는 기존 레플리카 셋은 계속 유지되고 새로운 레플리카 셋은 무한 루프를 돌며 이미지를 가져온다.
- 기존 레플리카 셋이 종료되지 않았기에 서비스는 기존 버전으로 계속해서 실행되고 있는 상태다.
- 그렇기에 문제를 천천히 찾아본 후, 수정하여 적용하면 된다.

### Deployment 구성 yaml 파일 예시

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  # Unique key of the Deployment instance
  name: deployment-example
spec:
  replicas: 3
  template:
    metadata:
      labels:
          app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
```

### 롤링 업데이트

- 새로운 버전으로 업데이트 할 때, 기존 버전이 중단되지 않도록 업데이트 하는 방법이다.
- 과정은 아래와 같다
  1. 새로운 레플리카 셋을 생성한다.
  2. 새로운 레플리카 셋이 준비되면 기존 레플리카 셋을 제거한다.
  3. 이때 기존 레플리카 셋 자체를 삭제하는 것이 아니라 레플리카 수를 0으로 만든다.
  4. 기존 레플리카 셋이 남아있기에 추후 롤백이 가능하다.
- 방법은 yaml 파일에서 버전만 변경하고 `apply` 명령을 실행하면 된다.
- 기존 레플리카 버전은 최대 10까지 보관한다.

### Deployment 명령어

- 기존 명령어와 동일하고 `deployment` 혹은 `deploy` 를 사용하면 된다.

#### Rollout 상태 확인

```bash
kubectl rollout status deployment {deployment-name}
```

#### Rollout 이력 확인

```bash
kubectl rollout history deployment {deployment-name}
```

#### Rollback

```bash
kubectl rollout undo deployment {deployment-name}

# 특정 버전 지정
kubectl rollout undo deployment {deployment-name} --to-revision={revision-number}
```

- Rollback은 유용하지만 롤백 후에 yaml 파일과 동일하지 않은 형태임을 인지해야 한다.
- 그렇기에 롤백 후에는 yaml 파일도 바로 수정해 주어야 한다.
- 그렇기에 긴급할 때(배포 후, 비정상 동작할 때와 같은)만 사용하는 것을 추천한다.


## Section 10: Networking and Services Discovery

### Networking

- 쿠버네티스 내에서도 두 개 이상의 컨테이너가 네트워크 동작을 해야 한다.
- 이를 위한 방법은 아래처럼 다양하다.

#### 1. 한 Pod 내부에 컨테이너 같이 넣기

- 한 Pod 내부에 여러 컨테이너를 넣어서 네트워크를 공유하는 방법이다.
- 각 컨테이너는 로컬호스트로 인식이 가능하다.
- 하지만 동일한 포드 내에서 어플리케이션과 데이터베이스를 같이 관리하면 포드 관리가 훨씬 복잡해지므로 잘 사용하지 않는다.
- 또한 포드에 문제가 생기면 어떤 컨테이너에서 문제가 생겼는지 원인을 찾기도 어렵다.
- 그렇기 때문에 하나의 포드에 두 개 이상의 컨테이너가 꼭 들어가야 하는 상황을 제외하고는 사용하지 않는다.

#### 2. kube-dns 서비스를 사용

- `kube-dns`
  - 쿠버네티스는 `kube-dns` 서비스를 백그라운드에서 자동으로 실행한다.
  - 이 서비스는 쿠버네티스 클러스터 내부에서 DNS 서비스를 제공한다.
  - dns 서버는 간단하게 말하면 여기서는 도메인 이름을 IP 주소로 변환해주는 서비스이다.
  - key-value 형태로 저장되어 있어서 key를 통해 value를 찾아준다.
- 서비스 간의 통신을 하면 kube-dns에서 가고자 하는 도메인을 통해 ip 주소를 얻어온다. 그리고 얻어 온 IP 주소를 통해 컨테이너와 통신한다.


### Namespace

- 네임스페이스는 쿠버네티스의 리소스를 별도의 영역으로 분할하는 방법이다.
- 예를 들어 여러 가지 포드들을 용도 별로 나누어서 front-end, back-end 등의 네임스페이스로 나눌 수 있다.
- 네임스페이스를 따로 설정하지 않으면 기본 네임스페이스인 `default` 에 할당되고 `kubectl get all` 을 실행했을 때도 `default` 네임스페이스에 있는 리소스만 출력된다.

#### 네임스페이스 명령어

- `namespace` 는 `ns` 라는 약어로 사용할 수 있다.
- 특정 네임스페이스 안에 있는 리소스를 사용하고 싶을 때는 `-n {namespace-name}` 을 붙여주면 된다.

```bash

1. 네임스페이스 목록

```bash
kubectl get namespaces
```

2. 네임스페이스 내부의 리소스 목록

```bash
kubectl get po -n {namespace-name}

kubectl get svc -n {namespace-name}

kubectl get all -n {namespace-name}
```

### Service Discovery

- 실행 중인 pod에 접속해서 아래와 같이 ip를 직접 확인할 수 있다.

```shell
> cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

- 이는 `kubectl get all` 에서 확인할 수 있는 ip와 동일하다.
- 또한 `nslookup` 을 통해 여러 서비스에 접근할 수 있는 것을 확인할 수 있다.

```shell
> nslookup fleetman-webapp
nslookup: can't resolve '(null)': Name does not resolve

Name:      fleetman-webapp
Address 1: 10.96.174.185 fleetman-webapp.default.svc.cluster.local

> nslookup database
nslookup: can't resolve '(null)': Name does not resolve

Name:      database
Address 1: 10.108.186.215 database.default.svc.cluster.local
```

- database의 실제 주소는 사실 `database.default.svc.cluster.local` 이다.
- 그렇기에 먼저 database 라는 이름을 가진 주소를 찾는다.
- 만약 없다면 `/etc/resolve.conf` 에 있는 `search` 항목을 하나씩 붙여가며 찾는다.
- FQDN(Fully Qualified Domain Name, 절대(전체) 도메인 네임)에서 `default` 는 `default` 네임스페이스를 의미한다.
- 그래서 `default` 네임스페이스에 속한 것이면 위처럼 이름만 적어서 찾고 다른 네임스페이스면 전체 도메인 이름을 사용하는 것이 좋다.
- 만약 다른 네임스페이스의 것을 찾고 싶다면 아래와 같이 네임스페이스를 붙여서 찾아야 한다.

```shell
> nslookup {service name}.{namespace name}
```

## Section 11: Microservice Architecture

### Monolith

- 모노리스 아키텍처는 전통적인 방식으로 모든 기능을 하나의 애플리케이션에 넣는 방식이다.
- 만약 상품에 대한 앱이라면 상품 정보, 주문, 결제, 카탈로그, 사용자 정보, 백엔드 서비스 등 모든 것을 하나의 앱에 넣는 방식이다.
- 그리고 이는 시간이 지날수록 더욱 커지고 복잡해진다.
- 결국에 특정 비지니스 영역을 다른 비지니스 영역을 건들지 않고 수정하기가 어려워진다.
- 이는 커다란 빌드 프로세스가 있어야 함을 의미한다.
- 그렇기에 처음 설계가 매우 중요하다.

### Microservice Architecture

- 모노리스 형식에 정반대라고 볼 수 있다. 즉 모든 기능을 작은 서비스로 나누는 방식이다.
- 마이크로 서비스의 중요한 점은 각 서비스가 독립적이라는 것이다.
- 마이크로 서비스는 각자 독립적으로 배포되고 실행되며 서로 교류하며 동작한다.
- 마이크로 서비스는 비지니스 영역에서 하나의 특정 영역만 처리해야 한다.
- Highly Cohesive, Loosely Coupled(높은 응집도, 낮은 결합도)를 지향한다.

### Microservice Database

- 대부분의 회사들은 통합 데이터베이스를 사용했을 것이다.
- 하지만 마이크로서비스는 통합 데이터베이스를 용납하지 않는다.
- 왜냐하면 서비스 간의 경계를 넘나들고 시스템의 어느 부분이라도 데이터베이스를 읽거나 쓸 수 있기 때문이다. 높은 응집도도 낮은 결합도도 지키지 못한다.
- 예를 들어 유저의 id, 비밀번호, 주소를 저장하는 데이터베이스를 마이크로서비스로 나눈다고 한다면 다음과 같이 나눌 수 있다.
  - 유저 정보 서비스: id, 비밀번호
  - 주소 정보 서비스: 주소

#### Bounded Context

- 거대한 모듈에서 다른 걸로 나누는 것을 Bounded Context라고 한다.
- 서비스는 만들기 전까지 하나의 서비스만 가지고 있는지 확인하기 어렵다.
- 그렇기에 서비스를 만들고 필요하다면 재구조화 하면 된다.
- 이런식으로 데이터베이스를 쪼개가면 된다.

---

### 실습 이미지들

- 백엔드
  - [Position Simulator](https://hub.docker.com/r/richardchesterwood/k8s-fleetman-position-simulator)
    - 파일에서 차량의 위치를 읽음, 그 후 큐로 데이터를 보냄
    - 메시지를 받아서 큐에 메시지를 보내기에 데이터를 받을 포트가 필요 없음, 따라서 **서비스를 사용하지 않음**
    - 그렇기에 간단한 파드로 배포해도 되지만 환경변수를 변경해야 하는 요구사항이 있다.
    - 또한 그냥 파드로 실행하면 여러 원인으로 종료되었을 때 다시 시작하지 않을 것이다.
    - 레플리카 셋도 방법이지만 추후 **서비스를 수정하면 롤링 배포를 해야 하기에 배포를 사용** 할 것이다.
  - [ActiveMQ](https://hub.docker.com/r/richardchesterwood/k8s-fleetman-queue)
    - 큐에 데이터를 받아서 처리함
    - 포트는 관리자 콘솔에 접근할 수 있도록 외부 포트를 뺀 것과 내부에서 컨테이너가 접근할 수 있는 포트 두 개를 염
  - [Position Tracker](https://hub.docker.com/r/richardchesterwood/k8s-fleetman-position-tracker): 큐의 위치를 읽고 위치와 관련된 다양한 계산을 수행함
    - 큐에서 받은 메시지를 읽고 REST 인터페이스를 노출한다.
    - 차량의 속도를 계산하고 차량의 경로를 저장한다.
    - 이를 통해 고객이 차량 서비스에 접근할 수 있다.
    - 컨테이너가 큐의 메시지를 받기 때문에 팬딩되던 메시지는 사라지고 확인되 메시지 숫자가 증가함을 확인할 수 있다.
- 프론트엔드
  - [API Gateway](https://hub.docker.com/r/richardchesterwood/k8s-fleetman-api-gateway)
    - 백엔드 서비스에 대한 요청을 받아서 적절한 서비스로 보내는 역할
    - 백엔드 마이크로서비스가 변하는 것을 막을 수 없기에 API Gateway를 사용
    - 프론트엔드는 절대로 백엔드 마이크로서비스를 직접적으로 접근해선 안 된다.
  - [Frontend Webapp](https://hub.docker.com/r/richardchesterwood/k8s-fleetman-webapp-angular): 자바스크립트 프론트엔드
    - 


### yaml 파일에 속한 리소스 모두 삭제

```Bash
kubectl delete -f {yaml-file / directory}
```

### pod에 환경변수 설정

```yaml
...
spec:
  containers:
    - name: position-simulator
      image: richardchesterwood/k8s-fleetman-position-simulator:release2
      env:  # 환경 변수 추가
        - name: SPRING_PROFILES_ACTIVE
          value: "rabbitmq"
```

### pod 로그 검사

```Bash
kubectl logs {pod-name}

# 실시간 로그 확인
kubectl logs -f {pod-name}
```

## Section 12: Kubernetes Persistence volume

### release2 버전 서비스의 문제점

- 차량의 경로가 포드 내부에서 실행되는 애플리케이션 내부의 메모리에 저장된다.
- Position Tracker 컨테이너는 결국 시간이 지남에 따라 메모리를 다 사용할 것이다.
- 메모리가 다 사용되 서비스가 충돌하여 종료되고 다시 실행되면서 저장 데이터가 사라질 것이다.
- 즉, Position Tracker가 차량의 속도를 계산하고 데이터를 저장하는 두 가지 목적을 가진 것이 마이크로서비스에서 잘못된 것이다.

### Persistence

- 퍼시스턴트 볼륨(PV)은 볼륨의 일종으로 PV를 사용하는 파드와 별개의 라이프사이클을 가진다. 
- 퍼시스턴트볼륨클레임(PVC)은 사용자가 스토리지에 요청하는 것을 말한다. 파드가 노드 리소스를 사용하듯이 PVC는 PV 리소스를 사용한다.
- 자세한 건 [공식문서](https://kubernetes.io/ko/docs/concepts/storage/persistent-volumes/)를 참고할 것

### MongoDB 연결하기

- MongoDB는 NoSQL에 일종이다.
- 위 문제를 해결하기 위해 몽고 db를 연결한 건데 두 가지를 진행해야 한다.
  - position tracker를 release3으로 업데이트 한다.
  - 몽고 db를 배포한다.
  - 이때 우리는 로컬에서 실행하기에 두 가지 순서가 크게 상관없지만 실제 배포 상황에서 진행된다면 몽고 db를 먼저 배포하고 release3으로 버전을 변경해야 한다. 그렇지 않으면 release3이 몽고 db를 찾지 못해 pod가 다운될 것이다.

- mongodb는 마이크로서비스와는 다른 별도의 서비스임으로 `workloads.yaml` , `service.yaml` 파일과 다른 별개의 파일에 구성한다.

### Volume mount

- 지금까지 몽고 컨테이너는 기본 환경설정으로 인해 메모리가 아니라 컨테이너 파일 시스템에 데이터를 저장하고 있었다. 이는 도커 컨테이너가 죽으면 데이터도 같이 사라진다는 의미다.
- 이를 우리의 하드 디스크에 저장하기 위해 도커의 볼륨과 비슷한 퍼시스턴트 볼륨이 있다.
- 배포하는 yaml 파일에서 우리는 아래와 같이 볼륨을 설정할 수 있다.

```yaml
# mongo-stack.yaml
...
  spec:
    containers:
      - name: mongodb
        image: mongo:3.6.5-jessie
        volumeMounts:
          - name: mongo-persistent-storage
            mountPath: /data/db # 컨테이너 내부에서 몽고가 데이터를 저장하는 경로
    volumes:
      - name: mongo-persistent-storage  # spec에서 volumeMounts의 이름으로 구분하여 마운트
        hostPath:  # 로컬 디스크를 활용, 테스트나 로컬 환경에서만 사용하는 옵션
          path: /mnt/some/directory/structure/  # 하드 디스크에 저장할 경로
          type: DirectoryOrCreate  # 없으면 폴더를 만듬
...
```

- 위 예시처럼 어떠한 마운트를 사용할 지, 타입을 정의할 수 있다.
- 이 외에도 [volume 공식 문서](https://kubernetes.io/docs/concepts/storage/volumes/)에 aws 등 여러 타입이 정의되어 있다.
- 컨테이너는 리눅스 환경을 사용하고 리눅스에는 mnt 폴더가 무조건 존재하기에 mnt 폴더 안에 생성했다.
- 허나 위 방법은 볼륨 마운트를 직접 실행한 것이다. 만약 포드가 수백개고 각 포드마다 볼륨 마운트가 존재한다면 클라우드를 이동할 때 여간 복잡하지 않을 수 없다.

### PersistentVolumeClaims

- 위와 같은 문제를 해결하기 위해 각 포드에 직접 설정하지 않고 환경파일을 따로 분리해서 환경파일에 정의할 수 있다.

```yaml
# mongo-stack.yaml
...
  spec:
    containers:
      - name: mongodb
        image: mongo:3.6.5-jessie
        volumeMounts:
          - name: mongo-persistent-storage
            mountPath: /data/db # 컨테이너 내부에서 몽고가 데이터를 저장하는 경로
    volumes:
      - name: mongo-persistent-storage  # spec에서 volumeMounts의 이름으로 구분하여 마운트
        # pointer to the configuration of HOW we want the mount to be implemented
        persistentVolumeClaims:
          claimName: mongo-pvc
...
```

```yaml
# storage.yaml

# What do you want? (무엇을 원하는가?)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongo-pvc
spec:
  storageClassName: mylocalstorage  # PV를 찾을 때 매칭할 라벨
  accessModes:
    - ReadWriteOnce  # 한 번에 다중 노드에 의해 마운트될 수 있는 볼륨을 원하는지에 대한 옵션
  resources:
    requests:
      storage: 20Gi  # db가 필요한 용량(물리적 저장소에 요구할 용량)

---

# How do we want it implemented? (어떻게 실행할 것인가?)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-storage  # 이름 정의(현재는 로컬을 사용할 거기에 local-storage라고 이름 정의)
spec:  # 어떤 방식으로 볼륨을 사용할 것인가
  storageClassName: mylocalstorage  # PVC와 매칭 시, 사용될 이름
  capacity:
    storage: 20Gi  # 볼륨이 제공할 저장소 용량(물리적 용량)
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/some/directory/structure/"
    type: DirectoryOrCreate
```

- 위와 같이 PVC를 통해 설정하게 된다면 만약 로컬에서 aws로 이동한다 해도 우리는 각 포드의 정의를 바꿔줄 필요가 없다.
- `PersistentVolumeClaim`
  - 우리가 볼륨에 원하는 것을 정의
  - 몽고 컨테이너에서 우리의 요구사항을 충족하기 위해 무엇을 원하는가 
  - 개발자의 관점
- `PersistentVolume`
  - 볼륨을 어떤 방법으로 실행할 지 결정한다.
  - 쿠버네티스 클러스터에서 어떤 물리적인 저장소를 원하는가
  - 쿠버네티스 관리자의 관점
- [`accessModes`](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)
  - 한 번에 다중 노드에 의해 마운트될 수 있는 볼륨을 원하는지에 대한 옵션
  - 옵션 내용은 문서 참고(대체로 default 값인 `ReadWriteOnce` 를 사용)
- 용량 옵션들은 로컬에선 그리 중요하지 않으나 클라우드 환경에선 얼마나 할당할 건지 정하는 것이기에 중요한 옵션이다.
- 또한 PV는 최소한 PVC에서 요구한 용량 정도는 가지고 있어야 한다. 만약 PVC가 더 큰 용량을 요구한다면 둘은 서로 매치되지 않는다.
- PVC는 `storageClassName` , `accessModes` , `storage` 를 고려하여 알맞은 PV를 찾는다. 이 과정을 **binding(결합)** 이라고 한다.
- `storage.yaml` 파일을 실행시키고 get all을 하면 볼륨이 보이지 않는다. 볼륨은 아래의 명령어로 직접 명시해야 보인다.

```Shell
> kubectl get pv
NAME            CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS     VOLUMEATTRIBUTESCLASS   REASON   AGE
local-storage   20Gi       RWO            Retain           Bound    default/mongo-pvc   mylocalstorage   <unset>                          98s

> kubectl get pvc
NAME        STATUS   VOLUME          CAPACITY   ACCESS MODES   STORAGECLASS     VOLUMEATTRIBUTESCLASS   AGE
mongo-pvc   Bound    local-storage   20Gi       RWO            mylocalstorage   <unset>                 3m31s

```

- 그러면 위처럼 mongo-pvc와 bound(결합)된 것을 확인할 수 있다.