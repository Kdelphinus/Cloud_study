# Logging and Monitoring (Section 20 - 22)

## Section 20: Kubernetes Cluster Logging

- 우리가 수동으로 로그를 확인할 때는 대체로 클러스터를 구성하고 실행하는 단계였다.
- 하지만 클러스터가 정상 동작을 할 때도 로그를 파악한다면 여러 정보를 얻을 수 있다.
- 그러나 로그가 너무 많기 때문에 포드 하나씩 로그를 직접 살펴볼 수는 없다.

### ELK 스택(Elastic Stack) 개념

#### 구성 서비스

1. Logstash or Fluentd

- 노드에 컨테이너들이 있을 때, 컨테이너들은 로그를 쓴다. 이러한 로그들을 모으는 프로그램이 logstash이다.
- 같은 노드에서 컨테이너로 존재한다.
- Fluentd 역시 Logstash와 동일한 역할을 수행하며 둘 중의 하나를 선택하는 형태로 스택이 구성된다.

2. Elasticsearch

- 엘라스틱서치는 분산 서치이고 분석 엔진이다.
- 즉, 엘라스틱서치로 어떤 데이터라도 엘라스틱서치 데이터베이스에 저장하고 쿼리할 수 있다.
- 서치 엔진 실행 파트로 봐도 된다.

3. Kibana

- 엘라스틱서치의 데이터를 시각화 해주는 도구다.
- 그래프, 차트 등 데이터를 정제하고 시각화하는 다양한 기능들이 있다.

#### 클러스터 설치

- 로그스태시는 클러스터에 있는 모든 싱글 노드에 설치해야 한다.
- 엘라스틱서치와 키바나는 원하는 방식으로 구성할 수 있다.
  - aws는 엘라스틱서치 서비스를 지원한다. 이를 통해 구성할 수도 있다. 이는 클러스터 밖에서 안정적이고 강력하게 실행할 수 있다는 장점이 있다.
  - 키바나는 웹 앱이기에 어느 곳에든 설치할 수 있다. 오직 키바나만 실행되는 ec2 인스턴스를 설정할 수도 있다.
  - 간단하고 빠르고 지저분한 방법은 쿠버네티스 클러스터에 직접 배포하는 방법이다.
    - 플루언티드는 각 노드에 설치되어야 한다.
    - 엘라스틱서치는 시스템에 적어도 2개의 노드에서 복제되도록 만들어 엘라스틱서치 서비스가 중단되지 않고 로그가 저장되지 않는 것을 방지한다.
    - 키바나는 임의의 싱글 노드에 설치한다.
    - 이 방법은 새로운 인스턴스를 사용하지 않기 때문에 간단하고 빠르다.
    - 하지만 노드에 포드가 계속해서 추가되기에 잠재적으로 노드에 과부하를 계속 준다.


### ELK 스택 설치

- 강사가 제공한 `elastic-stack.yaml` 과 `fluentd-config.yaml` 은 kubernetes 레포지토리에서 addon 폴더 안에 있는 것 중 fluentd 관련 배포 파일을 간략하게 만든 것이다.
- 이 두 파일을 ec2에 복사해서 배포하면 여러 포드들이 만들어지지만 `kubectl get po` 와 같은 명령어에서 잡히지 않는다.
- 왜냐하면 elk 스택은 `kube-system` 이라는 네임스페이스에 배포되었기 때문이다.
- 따라서 `kubectl get po -n kube-system` 명령어를 사용하면 다른 쿠버네티스 시스템 포드와 함께 동작 중임을 확인할 수 있다.

```Shell
NAME                                  READY   STATUS    RESTARTS   AGE
aws-node-6jtwf                        2/2     Running   0          43m
aws-node-fnmqk                        2/2     Running   0          43m
aws-node-j57fc                        2/2     Running   0          43m
coredns-f94fb47d9-9xx5j               1/1     Running   0          49m
coredns-f94fb47d9-br5bz               1/1     Running   0          49m
ebs-csi-controller-74dc77bcd9-hqt4c   6/6     Running   0          38m
ebs-csi-controller-74dc77bcd9-v7chw   6/6     Running   0          38m
ebs-csi-node-78gfv                    3/3     Running   0          38m
ebs-csi-node-b8s5c                    3/3     Running   0          38m
ebs-csi-node-qpwlq                    3/3     Running   0          38m
elasticsearch-logging-0               1/1     Running   0          3m47s
elasticsearch-logging-1               1/1     Running   0          3m14s
fluentd-es-v2.2.0-j8mk2               1/1     Running   0          3m47s
fluentd-es-v2.2.0-t5lpm               1/1     Running   0          3m47s
fluentd-es-v2.2.0-t9fdh               1/1     Running   0          3m47s
kibana-logging-758ff4b89f-5mmhj       1/1     Running   0          3m47s
kube-proxy-phpn5                      1/1     Running   0          43m
kube-proxy-rtmnr                      1/1     Running   0          43m
kube-proxy-vx9h7                      1/1     Running   0          43m
```

- 결과를 보면 `elasticsearch-logging-0/1` 이 생긴 것과 `fluentd` 가 각 노드별로 하나씩 총 3개가 생긴 것, `kibana` 가 생성된 것을 확인할 수 있다.
- `kubectl get svc -n kube-system` 명령어로 서비스를 확인하면 기존에 있는 `kube-dns` 말고 두 개가 더 생긴 것을 확인할 수 있다.

```Shell
NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP                                                                    PORT(S)                  AGE
elasticsearch-logging   ClusterIP      10.100.246.176   <none>                                                                         9200/TCP                 10m
kibana-logging          LoadBalancer   10.100.81.208    af59dff7d9f4b45889c536d218a177e0-1529305552.ap-northeast-2.elb.amazonaws.com   5601:32269/TCP           10m
kube-dns                ClusterIP      10.100.0.10      <none>                                                                         53/UDP,53/TCP,9153/TCP   56m
```

- `elasticsearch-loggin` 은 외부 ip없이 내부에서만 소통한다.
- `kibana-logging` 은 프론트 포드이기에 외부에서 접근할 수 있는 ip도 존재한다.
- `kubectl describe svc kibnan-logging -n kube-system` 명령어로 어떤 로드밸런서를 사용하는지 확인 후, 로드 밸런서 페이지에서 이를 확인한다.
- 로드 밸런서의 리스너를 살펴보면 프로토콜 포트에 할당된 포트를 확인할 수 있다. {로드밸런서 dns 이름}:{포트} 로 키바나 서비스에 접근할 수 있다.

### Kibana 사용

- 우선 우리는 Logging은 할 필요가 없다. 이미 엘라스틱 서치가 로깅을 하고 있다.
- 먼저 `Set up Index pattern` 에 들어가서 Index pattern을 정의한다.
  - logstash가 작동될 때 자동으로 엘라스틱 서치 인덱스를 생성한다.
  - 이때 이름은 `logstash-{날짜}` 인데 플루언티드가 인덱스로 logstash를 사용하는 이유는 호환성 때문일 것이다.
  - 그렇기에 날짜가 지날 때마다 인덱스가 계속 추가된다.
  - 정의 방법
    - 먼저 인덱스 패턴을 `logstash*` 으로 하여 자동으로 생성되는 인덱스를 하나로 묶는다.
    - 다음으로 플루언티드는 자동으로 모든 싱글 로그에 타임스탬프를 추가한다. 우리는 이 타임을 기반으로 해서 키바나가 데이터를 사용하도록 설정한다.
    - 이를 통해 인덱스 패턴을 생성한다.
- Discover 탭에 들어가면 검색 엔진을 볼 수 있다. 기본값은 검색 엔진에 아무것도 추가하지 않았기에 엘라스틱 서치에 있는 모든 데이터가 보여진다.
  - 예를 들어 'error' 를 검색하면 로그에 error가 찍힌 것들을 모아 볼 수가 있다.
  - 나온 결과를 클릭하면 다양한 정보들이 나오는데 `kubernetes.container_name` 으로 로그가 발생한 컨테이너의 이름을 알 수 있고 그 외에도 다양한 정보들이 기록되어 있다.
  - 또한 원본 로그 메시지 역시 확인할 수 있다.
  - 필터를 걸 수도 있는데 예를 들어 우리가 만든 앱에 관련된 로그만 확인하고 싶다면 `kube-system` 네임스페이스 결과를 제외해야 한다.
  - 이를 위해 'Add a filter' 버튼을 누르고 필터에서 `kubernetes.namespace_name` 을 선택해서 제외하거나 원하는 것만 나오게 만들 수 있다.
  - 또한 'Available Fields' 를 눌러 저장된 모든 필드를 확인할 수 있으며 각 필드별로 어떤 값이 어떤 비율로 분포하는지 간단하게 확인할 수 있다.
    - 이때 값에 있는 플러스 돋보기를 누르면 필터에 자동으로 추가된다.
  - 오류가 발생했을 때도 로그가 계속 쌓이기 때문에 굳이 쿠버네티스 시스템에 접속하지 않고도 문제를 확인할 수 있다.
  - 또한 필터의 설정을 저장하고 불러올 수 있다.
- Visualize 탭에 들어가면 여러 방식의 시각화를 만들 수 있다.
  - 예를 들어 Gauge 형식을 사용하면 네임스페이스나 저장된 필터 설정 중 하나를 고를 수 있다.
  - 이를 선택하면 그에 해당하는 로그가 발생한 횟수를 알려주는 시각화 데이터가 나온다.
  - 위에 시작 버튼을 누르면 주기적으로 새로고침을 하며 데이터를 확인할 수 있다.
  - 저장하여 대시보드에 추가할 수 있다.


## Section 21: Monitoring a Cluster

- 노드에 대한 모니터링은 aws를 사용한다면 기본적으로(무료) 인스턴스를 선택하고 모니터링 탭을 확인하면 존재한다.
  - 그리고 더 자세하게 분석을 제공하는 서비스가 있지만 이는 추가 요금이 발생한다.
- 이를 사용해도 되지만 클라우드를 옮기거나 할 때, 새로운 서비스에 다시 익숙해져야 하는 등의 단점이 발생한다.
- 그렇기에 자체 모니터링 서비스를 사용할 계획이다. 이는 더 다양한 모니터 테이블, 자체 알림 서비스까지 만들 예정이다.
- 다양한 도구들이 있지만 쿠버네티스에서 가장 많이 사용하는 것은 **프로메테우스** 다.

### Prometheus & Grafana

- 프로메테우스는 수많은 데이터를 모으는 역할을 한다.
- 그리고 그래픽 도구로 가장 흔히 사용되는 것이 그라파나이다.
- 이 두 개를 사용하는 것은 매우 흔하지만 이 서비스는 쿠버네티스만을 위한 서비스는 아니기에 이를 직접 구성하는 것은 깊고 어려운 주제이다.
- 그렇기에 이 강의에선 미리 만들어진 스택을 사용할 것이다.
  - 과거에는 [helm/charts github](https://github.com/helm/charts) 레포지토리 안에 stable/prometheus-operator 라는 스택을 사용했었다.
  - 그러나 이것이 prometheus-community로 옮겨가면서 현재는 [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)를 사용해야 한다.
  - 이 자체도 굉장히 깊은 내용이기에 강의에선 yaml 파일이 제공된다.

### Prometheus

- `crds.yaml` 파일은 공통 파일로 수많은 것들이 정의되어 있다.
- 그리고 다른 파일은 eks와 kops 중 어떤 것을 사용하느냐에 따라 둘 중 한 파일을 사용해야 한다.
  - 두 개의 차이를 둔 이유는 알림에서 약간의 차이가 있기 때문이고 이는 다음 섹션에서 확인할 예정이다.
  - 또한 eks는 마스터 노드가 없기에 접근할 필요가 없다. 허나 kops는 마스터 노드가 있기에 마스터 노드가 충돌되거나 잘못된 접근을 했을 때 알림이 필요하다.
- 주의할 점은 `crds.yaml` 파일을 무조건 먼저 적용해야 한다는 것이다. 먼저 적용을 마친 뒤, `monitoring.yaml` 파일을 적용해야 한다.

- 허나 직접 배포해보면 에러가 발생하고 'monitoring' 네임스페이스 안에 생성되지도 않는다.
- 그렇기에 아래는 강의가 아니라 직접 helm을 설치해 진행한 것이다.
- stack 설치는 [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)와 [블로그](https://leehosu.github.io/kube-prometheus-stack)를 참고했다.

```Shell
# helm 설치
$ curl -sSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash

# 설치 버전 확인
$ helm version --short
```

```Shell
# monitoring이란 네임스페이스 생성
$ kubectl create ns monitoring
```

```Shell
# 헬름 차트의 저장소 추가
$ helm repo add prometheus-community https://prometheus-community.github.io/
$ helm repo update
```

```Shell
# helm chart 배포
$ helm install prometheus -n monitoring prometheus-community/kube-prometheus-stack
```

```Shell
# service 확인
$ kubectl get svc -n monitoring
NAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
alertmanager-operated                     ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP   19s
prometheus-grafana                        ClusterIP   10.100.89.202    <none>        80/TCP                       22s
prometheus-kube-prometheus-alertmanager   ClusterIP   10.100.37.222    <none>        9093/TCP,8080/TCP            22s
prometheus-kube-prometheus-operator       ClusterIP   10.100.205.56    <none>        443/TCP                      22s
prometheus-kube-prometheus-prometheus     ClusterIP   10.100.191.234   <none>        9090/TCP,8080/TCP            22s
prometheus-kube-state-metrics             ClusterIP   10.100.62.196    <none>        8080/TCP                     22s
prometheus-operated                       ClusterIP   None             <none>        9090/TCP                     19s
prometheus-prometheus-node-exporter       ClusterIP   10.100.126.102   <none>        9100/TCP                     22s
```

- 동작되는 서비스 중에서 `alertmanager-operated` 같은 경우는 알림, `prometheus-kube-prometheus-prometheus` 는 자체 프론트를 지원한다.
- 만약 테스트를 위해 프로메테우스에 직접 접근해야 한다면 프로메테우스-프로메테우스에 접근해야 한다. 하지만 ClusterIP이기에 외부에서 접근할 수 없다.
- 이를 바꾸기 위해 로드 밸런서를 사용하면 빠르고 간편하지만 약간의 비용이 발생한다.
- 이번엔 로드 밸런서를 사용하는 방법말고 다른 방법을 사용한다.

```Shell
# prometheus-kube-prometheus-prometheus 서비스를 수정하는 명령어
$ kubectl edit svc prometheus-kube-prometheus-prometheus -n monitoring
```

- 허나 이 방법은 좋지는 않다. 실행되는 환경과 yaml 파일 간의 간극이 생기는 것이기 때문이다.
- 그렇기에 클러스터를 삭제했다가 다시 적용했을 때, edit 명령어는 적용되지 않은 상태일 것이다.
- 그렇기에 일시적으로 정확히 아는 상태에서만 사용해야 한다.

```yaml
...
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

- 가장 아래로 내려가면 위와 같이 ClusterIP로 되어있을 것이다. 이를 아래와 같이 수정한다.

```yaml
...
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer: {}
```

- 저장하고 나가면 적용되는데 만약 오타나 없는 명령어를 사용하면 다시 편집기로 돌아온다.
- 다시 서비스 목록을 출력하면 아래와 같이 로드 밸런서로 변한 것을 확인할 수 있다.

```Shell
$ kubectl get svc -n mornitoring
NAME                                      TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                         AGE
alertmanager-operated                     ClusterIP      None             <none>                                                                        9093/TCP,9094/TCP,9094/UDP      12m
prometheus-grafana                        ClusterIP      10.100.89.202    <none>                                                                        80/TCP                          12m
prometheus-kube-prometheus-alertmanager   ClusterIP      10.100.37.222    <none>                                                                        9093/TCP,8080/TCP               12m
prometheus-kube-prometheus-operator       ClusterIP      10.100.205.56    <none>                                                                        443/TCP                         12m
prometheus-kube-prometheus-prometheus     LoadBalancer   10.100.191.234   a433cb10462a1460ca59e24fe94d0fa9-726087349.ap-northeast-2.elb.amazonaws.com   9090:30435/TCP,8080:30872/TCP   12m
prometheus-kube-state-metrics             ClusterIP      10.100.62.196    <none>                                                                        8080/TCP                        12m
prometheus-operated                       ClusterIP      None             <none>                                                                        9090/TCP                        12m
prometheus-prometheus-node-exporter       ClusterIP      10.100.126.102   <none>                                                                        9100/TCP                        12m
```

- 로드 밸런서의 이름이 있으니 저 주소에 포트를 9090으로 하여 접속하면 프로메테우스 프론트에 접근할 수 있다.
- 프로메테우스엔 여러 매트릭스를 설정할 수 있는데 그 중 `node_load1/5/15` 가 있다. 이는 1/5/15분 직전의 cpu 평균 로드를 나타낸다.
- 매트릭스를 선택하고 그래프를 클릭하면 값들에 대한 그래프를 볼 수 있다.
- 하지만 우리는 프로메테우스를 프론트로 이용할 계획이 없으니 다시 `ClusterIP` 로 복구할 것이다.

### Grafana

- 일반적인 목적의 그래픽 도구이자 다양한 데이터 소스에 연결될 수 있다.
- 이미 목록에 `prometheus-grafana` 가 있다. 우리는 이에 접근할 수 있도록 `LoadBalancer` 로 바꿀 것이다.
- 바꾸는 방법은 위와 같이 `edit` 명령어를 사용한다.
- 80포트이기에 따로 포트를 지정할 필요는 없고 몇 분 정도 기다리면 그라파나에 접속할 수 있다.

- 기본 계정은 다음과 같다.(깃허브에 정보 있음)
  - id: admin
  - pw: prom-operator
- 스택에 이미 정의된 대시보드가 많다.
- 우리는 이중에 `USE Method/Node` 와 `USE Method/Cluster` 를 살펴볼 것이다.
  - U: Utilisation(이용, 특정 리소스가 얼마나 사용되는가)
  - S: Saturation(포화, 특정 리소스가 얼마나 오버로드 되는가)
  - E: Errors(에러, 예상치 못한 동작)
- 대시보드에 들어가면 바로 그래픽이 나타난다.
- USE 메소드 노드
  - 노드는 각 인스턴스 별로 나타나며 개인 IP로 식별하면 된다.
  - 이때 새로고침을 걸어두면 클러스터에 부하가 갈 수 있다. 괜찮은 성능이면 상관없지만 이미 어느 정도 부하가 있는 클러스터라면 더 악화시킬 수 있다.
  - 하지만 각 노드별로 확인해야 하기에 불편하다.
- USE 메소드 클러스터
  - 각 노드가 겹쳐서 나오기에 노드 별로 비교하기가 쉽다.
- Persistent Volumes
  - 시스템에 정의된 모든 퍼시스턴트 볼륨을 확인할 수 있다.


## Section 22: Alerts

- 우리가 직접 모니터링을 계속할 수 없기에 알림은 실제 프로젝트에서 매우 중요하다.

### 알림 소개 및 사용

- 만약 클러스터를 정지했다가 다시 켜서 프로메테우스의 포드들이 `ClusterIP` 상태라면 `prometheus-prometheus` 으로 끝나는 포드를 전과 같이 `LoadBalancer` 로 수정한다.
- 저번에 프로메테우스의 프론트 대신 그라파나의 프론트를 사용했다. 하지만 알림을 위해선 프로메테우스의 프론트에 접속해야 한다.

- 프로메테우스 프론트에 접근한 뒤, 상단에 있는 `Alerts` 에 들어가면 알림들이 있다.
- 이 알림들은 미리 스택에서 정의된 것들이다.
  - 각 알림들을 눌러보면 이에 대한 정의가 있는데 어떤 것을 수집하고 어떤 기준으로 알림을 주는지에 대한 설정이 정의되어 있다.
  - 알림들을 더 추가할 수 있으나 이 역시 깊기에 이번 강의에선 다루지 않음
  - 스택에서 미리 정의된 알림으로 대부분 커버 가능
  - 그 중 `Watchdog` 알림은 알림 파이프라인이 작동하는지 확인하기 위한 것으로 설계부터 항상 울리고 끌 수 없다.
  - 워치독이 사용되는 이유는 알림 자체가 크래시 나서 알림이 망가지진 않았는지 확인하는 용도로 사용한다.
  - 워치독 알림을 주기적으로 받아 알림 서비스가 잘 동작하고 있는지 확인할 수 있다. 이에 대해 추후 살펴본다.


### 알림 받기

- 메일, 문자 등 다양한 방법이 있지만 가장 쉬운 방법은 슬랙으로 받는 것이다.
- 슬랙 채널을 만들고 앱을 추가해야 한다.
  - 수신 웹후크(Incoming Webhook): 외부에서 슬랙 채널로 전송
  - 송신 웹후크(Outgoing Webhook): 슬랙에서 외부로 전송
- 우리는 외부에서 받을 것이기에 수신 웹후크를 추가한다.
- 그러면 웹후크 URL가 설명이 주어지는데 URL을 통해 채널에 메시지를 전송하므로 이것이 공개되지 않도록 해야 한다.

```Shell
$ curl -X POST --data-urlencode "payload={\"channel\": \"#{Channnel Name}\", \"username\": \"webhookbot\", \"text\": \"This is posted to #alerts and comes from a bot named webhookbot.\", \"icon_emoji\": \":ghost:\"}" {Webhook URL}
ok%
```

- 위 명령어를 사용하면 ok가 출력되고 슬랙에 메시지가 보내졌음을 확인할 수 있다.

### 프로메테우스와 연결하기(alert manager)

- 프로메테우스에 있는 전체 알림 매니저는 환경 설정 파일이 관리한다.
- 이에 대한 설명은 [프로메테우스 알림 공식 문서](https://prometheus.io/docs/alerting/latest/configuration/)에 있는데 추후 프로젝트에서 아주 복잡한 요청이 있을 때 참고하면 된다.

```Shell
$ kubectl get poo -n monitoring
NAME                                                     READY   STATUS    RESTARTS   AGE
alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          58m
prometheus-grafana-d5679d5d7-fwxfv                       3/3     Running   0          59m
prometheus-kube-prometheus-operator-b7c4589c8-hrd9l      1/1     Running   0          59m
prometheus-kube-state-metrics-5b684b7487-d5xpm           1/1     Running   0          59m
prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          58m
prometheus-prometheus-node-exporter-rksb2                1/1     Running   0          59m
prometheus-prometheus-node-exporter-wf6qt                1/1     Running   0          59m
prometheus-prometheus-node-exporter-znxmt                1/1     Running   0          59m
```

- 위에서 `alertmanager-...` 으로 되어있는 것이 모든 걸 실행하는 포드이다.
- 우리는 config 파일을 `alertmanager` 에 보낼 수 있도록 명령어를 사용할 것이다. 이 명령어에 대한 개념은 추후에 나온다.

```Shell
$ kubectl get secret -n monitoring
NAME                                                                TYPE                 DATA   AGE
alertmanager-prometheus-kube-prometheus-alertmanager                Opaque               1      61m
alertmanager-prometheus-kube-prometheus-alertmanager-generated      Opaque               1      61m
alertmanager-prometheus-kube-prometheus-alertmanager-tls-assets-0   Opaque               0      61m
alertmanager-prometheus-kube-prometheus-alertmanager-web-config     Opaque               1      61m
prometheus-grafana                                                  Opaque               3      61m
prometheus-kube-prometheus-admission                                Opaque               3      61m
prometheus-prometheus-kube-prometheus-prometheus                    Opaque               1      61m
prometheus-prometheus-kube-prometheus-prometheus-tls-assets-0       Opaque               1      61m
prometheus-prometheus-kube-prometheus-prometheus-web-config         Opaque               1      61m
sh.helm.release.v1.prometheus.v1                                    helm.sh/release.v1   1      61m
```

- 우선 각각의 시크릿을 작업해야 하는 모니터링 시스템의 다른 부분에 있는 데이터 조각 정도로 생각하면 된다.
- 이 중 가장 위에 있는 `alertmanager-prometheus-kube-prometheus-alertmanager` 이 컨피그 파일이다.

```Shell
# yaml 파일 형식으로 시크릿을 출력
$ kubectl get secret -n monitoring alertmanager-prometheus-kube-prometheus-alertmanager -o yaml
apiVersion: v1
data:
  alertmanager.yaml: Z2xvYmFsOgogIHJlc29sdmVfdGltZW91dDogNW0KaW5oaWJpdF9ydWxlczoKLSBlcXVhbDoKICAtIG5hbWVzcGFjZQogIC0gYWxlcnRuYW1lCiAgc291cmNlX21hdGNoZXJzOgogIC0gc2V2ZXJpdHkgPSBjcml0aWNhbAogIHRhcmdldF9tYXRjaGVyczoKICAtIHNldmVyaXR5ID1+IHdhcm5pbmd8aW5mbwotIGVxdWFsOgogIC0gbmFtZXNwYWNlCiAgLSBhbGVydG5hbWUKICBzb3VyY2VfbWF0Y2hlcnM6CiAgLSBzZXZlcml0eSA9IHdhcm5pbmcKICB0YXJnZXRfbWF0Y2hlcnM6CiAgLSBzZXZlcml0eSA9IGluZm8KLSBlcXVhbDoKICAtIG5hbWVzcGFjZQogIHNvdXJjZV9tYXRjaGVyczoKICAtIGFsZXJ0bmFtZSA9IEluZm9JbmhpYml0b3IKICB0YXJnZXRfbWF0Y2hlcnM6CiAgLSBzZXZlcml0eSA9IGluZm8KLSB0YXJnZXRfbWF0Y2hlcnM6CiAgLSBhbGVydG5hbWUgPSBJbmZvSW5oaWJpdG9yCnJlY2VpdmVyczoKLSBuYW1lOiAibnVsbCIKcm91dGU6CiAgZ3JvdXBfYnk6CiAgLSBuYW1lc3BhY2UKICBncm91cF9pbnRlcnZhbDogNW0KICBncm91cF93YWl0OiAzMHMKICByZWNlaXZlcjogIm51bGwiCiAgcmVwZWF0X2ludGVydmFsOiAxMmgKICByb3V0ZXM6CiAgLSBtYXRjaGVyczoKICAgIC0gYWxlcnRuYW1lID0gIldhdGNoZG9nIgogICAgcmVjZWl2ZXI6ICJudWxsIgp0ZW1wbGF0ZXM6Ci0gL2V0Yy9hbGVydG1hbmFnZXIvY29uZmlnLyoudG1wbA==
kind: Secret
metadata:
  annotations:
    meta.helm.sh/release-name: prometheus
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2024-06-05T07:10:54Z"
  labels:
    app: kube-prometheus-stack-alertmanager
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 59.1.0
    chart: kube-prometheus-stack-59.1.0
    heritage: Helm
    release: prometheus
  name: alertmanager-prometheus-kube-prometheus-alertmanager
  namespace: monitoring
  resourceVersion: "4409"
  uid: 5461a533-1c92-4c2c-bfd3-3b3dd2ce7a63
type: Opaque
```

- 여기서 우리가 관심있는 부분은 `alertmanager.yaml` 뒤에 있는 부분인데 이는 base64 형식으로 인코딩된 것이다. 이를 아래 명령어로 다시 디코딩할 수도 있다.

```Shell
$ echo {alertmanager.yaml의 값} | base64 -d 
global:
  resolve_timeout: 5m
inhibit_rules:
- equal:
  - namespace
  - alertname
  source_matchers:
  - severity = critical
  target_matchers:
  - severity =~ warning|info
- equal:
  - namespace
  - alertname
  source_matchers:
  - severity = warning
  target_matchers:
  - severity = info
- equal:
  - namespace
  source_matchers:
  - alertname = InfoInhibitor
  target_matchers:
  - severity = info
- target_matchers:
  - alertname = InfoInhibitor
receivers:
- name: "null"
route:
  group_by:
  - namespace
  group_interval: 5m
  group_wait: 30s
  receiver: "null"
  repeat_interval: 12h
  routes:
  - matchers:
    - alertname = "Watchdog"
    receiver: "null"
templates:
- /etc/alertmanager/config/*.tmpl
```

- 완전한 컨피그 파일은 아닐 수도 있지만 현재 알림을 받지 않도록 되어 있는 것을 확인할 수 있다.
- 그렇기에 우리는 이 시크릿을 지우고 우리가 만든 컨피그 파일을 추가할 것이다.
- 우선 주어진 파일을 ec2로 옮길 것인데 이름은 반드시 `alertmanager.yaml` 이 되어야 한다.

```Shell
# 기존 alertmanager secret 제거
$ kubectl delete secret -n monitoring {alertmanager 시크릿 이름}
```

```Shell
# 우리 파일로 secret 생성
$ kubectl create secret generic --from-file=alertmanager.yaml -n monitoring {alertmanager 시크릿 이름}
```

- 그러면 10분 안에 슬랙에 메시지가 도착할 것이다.
- 시크릿은 포드에 의해 선택도리 수 있는 데이터 조각이다. 이번에는 알림 매니저 포드가 이 시크릿을 선택한 것이다.

### Alert manager 연결 중 문제 발생 시 해결 방법

- 우선 `curl` 로 슬랙 채널에 제대로 보내지는지 확인
  - 안 보내진다면 채널 알림이나 URL을 다시 확인해 봐야 한다.
- 만약 `alertmanger.yaml` 파일에 문제가 있다면(오타, 문법 오류 등) 알림만 오지 않을 뿐 확인할 수 없다.
  - 이때 `describe` 명령어를 쓸 수가 없다. 그렇기에 로그를 확인해야 한다.
  ```Shell
  $ kubectl logs -f -n monitoring {alert manager pod의 이름}
  ```
  - 만약 알림이 잘못되었다면 로그가 아니라 에러가 발생한다. 컨테이너 이름을 특정하라는 에러가 발생한다.
  ```Shell
  $ kubectl get po -n monitoring
  NAME                                                     READY   STATUS    RESTARTS   AGE
  alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          88m
  prometheus-grafana-d5679d5d7-fwxfv                       3/3     Running   0          88m
  prometheus-kube-prometheus-operator-b7c4589c8-hrd9l      1/1     Running   0          88m
  prometheus-kube-state-metrics-5b684b7487-d5xpm           1/1     Running   0          88m
  prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          88m
  prometheus-prometheus-node-exporter-rksb2                1/1     Running   0          88m
  prometheus-prometheus-node-exporter-wf6qt                1/1     Running   0          88m
  prometheus-prometheus-node-exporter-znxmt                1/1     Running   0          88m
  ```
  - 포드를 다시 살펴보면 `READY` 부분에 알림 매니저는 2로 표기되어있는 것을 확인할 수 있다. 이는 포드 안에 컨테이너가 2개 있다는 것을 말한다.
    - 첫 번째 컨테이너는 알림 매니저 컨테이너로 알림 매니저가 하는 모든 작엄을 수행한다.
    - 두 번째 컨테이너는 헬퍼 컨테이너라고 생각하면 된다.
      - 무한 루프가 실행되며 시크릿이 바뀌었는지 끊임없이 확인한다.
      - 시크릿 변경이 발견되면 다른 컨테이너에게 무슨 일이 생겼는지 알려주고 다시 로드하도록 만든다.
      - 즉, 컨테이너가 2개이기에 어떤 컨테이너의 로그를 원하는지 에러가 발생하는 것이다.
  - 그렇기에 우리는 에러문에서 보여주는 컨테이너 이름을 보고 특정 컨테이너의 로그를 보여달라고 특정해야 한다.
  ```Shell
  $ kubectl logs -f -n monitoring {alert manager pod의 이름} -c {컨테이너 이름}
  ```
  - 로그를 통해 에러를 확인하여 수정하면 된다.

> 허나 현재 내가 했을 땐, 로그를 불러오는데 에러도 발생하지 않고 로그에 에러도 없다.
> 왜 그런지 더 찾아봐야 할 것 같은데 강의는 알림 매니저 secret이 하나인 것에 반해 나는 여러 개인 것을 볼 때 이 차이인가 싶기도 하다. 


### 경고 발생 처리

- 테스트를 위해 알림을 고의로 생성하는 것은 쉽지는 않다. 예를 들어 파드, 노드 등은 삭제해도 각각 쿠버네티스와 아마존에 의해 재동작하기에 알림으로 넘어가지 않는다.
- 즉, 정상적인 클러스터에선 알림을 발생시키가 매우 어렵다. (그만큼 안정적이란 의미이다.)
- 이를 위해 미리 준비된 `bad-pod.yaml` 파일을 사용하여 계속해서 파드가 크래시 나도록 만든다.

```Shell
$ kubectl get po
NAME                                  READY   STATUS              RESTARTS      AGE
api-gateway-56c46fbcdb-c68xz          1/1     Running             0             118m
bad-deployment-7f96555b89-qvvwx       0/1     CrashLoopBackOff    3 (13s ago)   61s
bad-deployment-7f96555b89-wrwgj       0/1     RunContainerError   3 (14s ago)   61s
mongodb-57bd58556b-mss5t              1/1     Running             0             118m
position-simulator-5fdb4ddbd5-xdl7z   1/1     Running             0             118m
position-tracker-59fdfd8cf4-hn7h2     1/1     Running             0             118m
queue-7f49849f96-mtq8r                1/1     Running             0             118m
webapp-66765b68df-7hqn7               1/1     Running             0             118m
webapp-66765b68df-nd8ql               1/1     Running             0             118m
```

- 위를 보면 `CrashLooopBackOff` 가 발생한 것을 볼 수 있는데 이는 컨테이너가 반복적으로 실행 실패하는 상태를 의미한다.
- 이를 프로메테우스 알림에서 확인하면 pending 된 3개의 알림을 확인할 수 있다.
- `KubePodCrashLooping` 알림을 보면 조건이 15분 동안 만족해야 함을 알 수 있다.
  - 만약 15분 내로 파드가 다시 정상 작동되면 pending 상태는 해제되고 알림을 받지 않는다.
  - 하지만 15분이 넘어가면 알림이 발생한다.
- 이 알림을 제거하는 방법은 문제를 해결하는 것이다.

```Shell
$ kubectl delete -f bad-pod.yaml
```

- 이렇게 포드를 제거하면 프로메테우스 알림 창에 경고들이 사라지는 것을 확인할 수 있다.
- [RESOLVED] 라는 메시지가 해결되었음을 확인시켜준다.


### 알림을 무음으로 설정하는 방법

- 이번엔 `alertmanager` 로 끝나는 서비스에 접근한다.

```Shell
$ kubectl get svc -n monitoring
NAME                                      TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                         AGE
alertmanager-operated                     ClusterIP      None             <none>                                                                        9093/TCP,9094/TCP,9094/UDP      145m
prometheus-grafana                        ClusterIP      10.100.90.221    <none>                                                                        80/TCP                          145m
prometheus-kube-prometheus-alertmanager   ClusterIP      10.100.100.231   <none>                                                                        9093/TCP,8080/TCP               145m
prometheus-kube-prometheus-operator       ClusterIP      10.100.247.247   <none>                                                                        443/TCP                         145m
prometheus-kube-prometheus-prometheus     LoadBalancer   10.100.171.203   a32ff1cb0e4cd48109e5c5bd30fa57d2-467620448.ap-northeast-2.elb.amazonaws.com   9090:31956/TCP,8080:31276/TCP   145m
prometheus-kube-state-metrics             ClusterIP      10.100.109.145   <none>                                                                        8080/TCP                        145m
prometheus-operated                       ClusterIP      None             <none>                                                                        9090/TCP                        145m
prometheus-prometheus-node-exporter       ClusterIP      10.100.24.226    <none>                                                                        9100/TCP                        145m
```

```Shell
$ kubectl edit svc -n monitoring {alertmanager로 끝나는 서비스 이름} 
```

- 이전처럼 ClusterIP를 LoadBalancer로 수정한다.
- 로드 밸런서가 연결되면 주어진 링크와 포트로 접속한다.
- 여기에는 프로메테우스 프론트에는 없는 알림 끄기 기능이 있다.
- 일시적인 기간만 설정하거나 아예 무음으로 설정할 수도 있다.
- 이건 정확히는 알림은 발생하지만 슬랙으로 알림이 전달되지 않는다.

- 알림 무음이 필요한 이유는 우리가 업데이트든, 테스트든 일부러 알림이 발생할만한 일을 할 때, 다른 엔지니어에게 알림이 안 가도록 하기 위해 필요하다.
- 'New Silence'를 클릭하고 'Matchers'에 'alertname="{알림 이름}"' 을 입력한다.
- 'Creator' 는 만든 사람, 'Comment'는 왜 이 알림을 만들었는지 이유를 적으면 된다. 

### Watchdog

- 워치독은 알림 매니저가 잘 작동하는지 살펴보는 역할을 한다.
- 워치독은 항상 실행되어야 하며 알림 발생을 방지할 수도 없다.
- 이미 슬랙으로 워치독 알림을 받고 있지만 이는 워치독 알림만 너무 쌓여서 다른 알림이 가려지는 문제가 있다.
- 워치독은 클러스터와 완전 다른 곳에 있어야 한다. aws에선 다른 리전, 아니면 다른 하드웨어에서 워치독을 받는 방법이 있다.
- 아니면 [Dead Man's Snitch](https://deadmanssnitch.com/)를 사용할 수 있다.
  - 이는 크론탭 등 주기적으로 동작하는 서비스가 잘 동작하는지 확인하는 서비스이다.
  - 이를 통해 워치독이 일정 기간 알림이 안 오면 알림을 받게 만들 수 있다.
  - 데드 맨즈 스니치가 어디서 실행되는지 우리는 알 수 없기에 만약 이것이 같은 리전에 있는 것이었다면 리전이 다운되었을 때 알림 역시 무력화된다.
  - 이를 확인하는 방법들을 찾아 우리가 사용하는 리전과 데드 맨즈 스니치가 사용하는 리전 혹은 서비스가 겹치지 않게 하는 것이 안전하다.


### PagerDuty

- 페이저 듀티는 전화나 sms로 알림을 보내주는 서비스다.
- 이를 통해 워치독은 워치독으로, 나머지 알림은 sms로 가게 만들 수 있다.
- `sample-alertmanger.yaml` 파일이 페이저 듀티와 데드 맨즈 스니치를 사용하는 yaml 파일이다.

```yaml
# sample-alertmanager.yaml
...
route:
  group_by:
    - alertname
  group_interval: 1m
  group_wait: 5s
...
```

- 위 파일에서 `group` 관련 옵션은 우리가 받을 알림을 줄여주는 역할을 한다.
- 예를 들어 리전 자체에 문제가 생겨서 거기있는 300개의 노드가 종료되면 우리는 300개 이상의 알림을 받을 것이다. 하지만 이 문제는 리전 하나의 문제이고 우리는 알림을 하나만 받기를 원한다. 이를 해결하기 위해 있는 옵션이다.
- `group_by` : 어느 알림이든 이름이 똑같고 특정 간격으로 알림이 온다면 모두 중복된 알림으로 간주한다.
- `group_interval` : 어느 정도 간격으로 알림을 중복으로 묶을 것인가
- `group_wait` : 첫번째 알림이 활성화되고 알림을 보내기까지 기다리는 시간

